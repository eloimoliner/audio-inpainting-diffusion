#network from the paper: Solving audio inverse problems with a diffusion model
name: "unet_cqt_oct_with_attention"
callable: "networks.unet_cqt_oct_with_projattention_adaLN_2.Unet_CQT_oct_with_attention"


use_fencoding: False
use_norm: True

filter_out_cqt_DC_Nyq: True

depth: 7 #total depth of the network (including the first stage)

emb_dim: 256 #dimensionality of the RFF embeddings


#dimensions of the first stage (the length of this vector should be equal to num_octs)
#Ns: [64, 96 ,96, 128, 128,256, 256] #it is hardcoded
Ns: [64,96, 96, 128, 128,256, 256] #it is hardcoded
#Ns: [8, 8 ,8, 8, 16,16, 16] #it is hardcoded

attention_layers: [0, 0, 0, 0, 1, 1, 1, 1] #num_octs+bottleneck
#attention_Ns: [0, 0, 0, 0,256 ,512,1024 ,1024]

#Ns: [8,8,16,16,32,32,64] 
Ss: [2,2,2, 2, 2, 2, 2] #downsample factors at the first stage, now it is ignored

num_dils: [2,3,4,5,6,7,7]

cqt:
    window: "kaiser"
    beta: 1
    num_octs: 7
    bins_per_oct: 64 #this needs to be lower than 64, otherwise the time attention is inpractical


#inner_Ns: [64, 64, 64, 64]
#if 4x2, then down factor of 16!

bottleneck_type: "res_dil_convs"

num_bottleneck_layers: 1
#transformer:
#    num_heads: 8
#    dim_head: 64
#    num_layers: 16
#    channels: 512
#    attn_dropout: 0.1
#    multiplier_ff: 4
#    activation: "gelu" #fixed



#for now, only the last two layers have attention + bottleneck


attention_dict:
    num_heads: 8
    attn_dropout: 0.0
    bias_qkv: False
    N: 0
    rel_pos_num_buckets: 32
    rel_pos_max_distance: 64
    use_rel_pos: False
    Nproj: 8
    #the number of channels is the same as the Ns of the corresponding layer




